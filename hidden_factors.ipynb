{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=gpu1\"\n",
    "os.environ[\"CPATH\"] = \"\"\n",
    "from lasagnekit.generative.autoencoder import Autoencoder, greedy_learn_with_validation\n",
    "\n",
    "from lasagnekit.easy import BatchOptimizer, LightweightModel\n",
    "from lasagnekit.datasets.mnist import MNIST\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from lasagne import layers, updates, init, nonlinearities\n",
    "import theano.tensor as T\n",
    "from theano.sandbox import rng_mrg\n",
    "import theano\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lasagne.layers import get_all_layers\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.filter import threshold_otsu\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lasagnekit import easy\n",
    "\n",
    "from lasagnekit.generative.capsule import Capsule\n",
    "from lasagnekit.easy import BatchIterator\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from lasagnekit.easy import BatchOptimizer, LightweightModel\n",
    "from lasagne import init\n",
    "from collections import OrderedDict\n",
    "from lasagne import init, layers, updates, nonlinearities\n",
    "from lasagne.layers.helper import get_all_layers\n",
    "from lasagne.layers import helper\n",
    "import theano.tensor as T\n",
    "from theano.sandbox import rng_mrg\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from lasagnekit.datasets.fonts import Fonts\n",
    "import theano\n",
    "from collections import OrderedDict\n",
    "import theano.tensor as T\n",
    "\n",
    "from lasagnekit.generative.capsule import Capsule\n",
    "from lasagne.layers import Layer\n",
    "from lasagnekit.datasets.cached import Cached\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from skimage.filter import threshold_otsu\n",
    "from skimage.transform import resize\n",
    "from lasagne.layers import cuda_convnet\n",
    "#from lasagne.layers.dnn import Conv2DDNNLayer\n",
    "from lasagne.layers import Conv2DLayer\n",
    "from lasagnekit.datasets.rescaled import Rescaled\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def binarize(X):\n",
    "    X_b = np.empty(X.shape, dtype=X.dtype)\n",
    "    for i in range(X.shape[0]):\n",
    "        X_b[i] = 1. * (X[i] <= threshold_otsu(X[i]))\n",
    "    return X_b\n",
    "\n",
    "def resize_all(X, w, h):\n",
    "    if X.shape[1] == w and X.shape[2] == h:\n",
    "        return X\n",
    "    X_b = np.empty((X.shape[0], w, h), dtype=X.dtype)\n",
    "    for i in range(X.shape[0]):\n",
    "        X_b[i] = resize(X[i], (w, h))\n",
    "    return X_b  \n",
    "\n",
    "class SumLayer(Layer):\n",
    "    def __init__(self, \n",
    "                 incoming,\n",
    "                 axis=1,\n",
    "                 **kwargs):\n",
    "        super(SumLayer, self).__init__(incoming, **kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return input.sum(axis=self.axis)\n",
    "    \n",
    "    def get_output_shape_for(self, input):\n",
    "        shape = list(self.input_shape)\n",
    "        del shape[self.axis]\n",
    "        return tuple(shape)\n",
    "\n",
    "class Model:\n",
    "    def get_all_params(self, **t):\n",
    "        return list(set(self.x_to_z.get_all_params(**t) + \n",
    "                        self.x_to_y.get_all_params(**t) + \n",
    "                        self.z_to_x.get_all_params(**t)))\n",
    "\n",
    "def cross_correlation(a, b):\n",
    "    a = a - a.mean(axis=0)\n",
    "    b = b - b.mean(axis=0)\n",
    "    return 0.5 * ((((a.dimshuffle(0, 'x', 1) * b.dimshuffle(0, 1, 'x'))).mean(axis=0))**2).sum()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = \"fonts\"\n",
    "w, h = 28, 28 # Desired resolution (if necessary the images will be resize to fit w and h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if dataset == \"mnist\":\n",
    "    data = MNIST()\n",
    "elif dataset == \"fonts\":\n",
    "    data = Fonts(kind=\"all_64\", \n",
    "                 labels_kind=\"letters\")\n",
    "\n",
    "data = Cached(Rescaled(data, (w, h)))\n",
    "data.load()\n",
    "\n",
    "X = data.X\n",
    "y = data.y\n",
    "output_dim = data.output_dim\n",
    "real_w, real_h = data.img_dim\n",
    "    \n",
    "y = label_binarize(y, np.arange(output_dim))\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape, y.shape, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if dataset == \"fonts\":\n",
    "    pass\n",
    "    #filter_by_letter = (y[:, 19]==1) | (y[:, 20]==1)\n",
    "    #X = X[filter_by_letter]\n",
    "    #y = y[filter_by_letter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "plt.imshow(X[20].reshape((w, h)), cmap=\"gray\")\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y)\n",
    "train, test = train_test_split(range(X.shape[0]), test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "nb_samples_for_learning_curve = 100\n",
    "nb_tries_learning_curve = 10\n",
    "\n",
    "class MyBatchOptimizer(BatchOptimizer):\n",
    "    \n",
    "    def iter_update(self, epoch, nb_batches, iter_update_batch):\n",
    "        status = super(MyBatchOptimizer, self).iter_update(epoch, nb_batches, iter_update_batch)\n",
    "        \n",
    "        all_of_them = [\n",
    "            \"acc_train\",\n",
    "            \"acc_valid\",\n",
    "            \"rec_train\",\n",
    "            \"rec_valid\",\n",
    "            \"crosscor_train\",\n",
    "            \"crosscor_valid\"\n",
    "        ]\n",
    "        for a in all_of_them:\n",
    "            status[a] = 0.\n",
    "        for i in range(nb_tries_learning_curve):\n",
    "        \n",
    "            s = np.arange(len(train))\n",
    "            np.random.shuffle(s)\n",
    "            s_train = s[0:nb_samples_for_learning_curve]\n",
    "\n",
    "\n",
    "            s = np.arange(len(test))\n",
    "            np.random.shuffle(s)\n",
    "            s_test = s[0:nb_samples_for_learning_curve]\n",
    "\n",
    "\n",
    "            status[\"acc_train\"] += (self.model.predict(X[train][s_train])==y[train][s_train].argmax(axis=1)).mean()\n",
    "            status[\"acc_valid\"] += (self.model.predict(X[test][s_test])==y[test][s_test].argmax(axis=1)).mean()\n",
    "\n",
    "            status[\"rec_train\"] += self.model.get_reconstruction_error(X[train][s_train])\n",
    "            status[\"rec_valid\"] += self.model.get_reconstruction_error(X[test][s_test])\n",
    "\n",
    "            status[\"crosscor_train\"] += self.model.get_cross_correlation(X[train][s_train])\n",
    "            status[\"crosscor_valid\"] += self.model.get_cross_correlation(X[test][s_test])\n",
    "        for a in all_of_them:\n",
    "            status[a] /= nb_tries_learning_curve\n",
    "        return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_type = \"convnet\" # or \"convnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if model_type == \"fully_connected\":\n",
    "    ## fully connected\n",
    "    latent_size = 4\n",
    "    num_hidden_units = 1000\n",
    "\n",
    "    l_in = layers.InputLayer((None, w*h))\n",
    "    input_dim = w*h\n",
    "    output_dim = y.shape[1]\n",
    "\n",
    "    # encoder\n",
    "    l_encoder1 = layers.DenseLayer(l_in, num_units=num_hidden_units)\n",
    "    l_encoder2 = layers.DenseLayer(l_encoder1, num_units=num_hidden_units)\n",
    "    l_encoder3 = layers.DenseLayer(l_encoder2, num_units=num_hidden_units)\n",
    "    l_encoder4 = layers.DenseLayer(l_encoder3, num_units=num_hidden_units)\n",
    "    l_encoder4 = layers.DenseLayer(l_encoder4, num_units=num_hidden_units)\n",
    "\n",
    "\n",
    "    # learned representation\n",
    "    l_observed = layers.DenseLayer(l_encoder4, num_units=output_dim,\n",
    "                                      nonlinearity=T.nnet.softmax)\n",
    "\n",
    "    l_latent = layers.DenseLayer(l_encoder4, \n",
    "                                 num_units=latent_size,\n",
    "                                 nonlinearity=None) # linear\n",
    "\n",
    "    l_representation = layers.concat([l_observed, l_latent])\n",
    "\n",
    "    # decoder\n",
    "    l_decoder1 = layers.DenseLayer(l_representation, num_units=num_hidden_units)\n",
    "    l_decoder2 = layers.DenseLayer(l_decoder1, num_units=num_hidden_units)\n",
    "    l_decoder3 = layers.DenseLayer(l_decoder2, num_units=num_hidden_units)\n",
    "    l_decoder4 = layers.DenseLayer(l_decoder3, num_units=num_hidden_units)\n",
    "    l_encoder4 = layers.DenseLayer(l_encoder4, num_units=num_hidden_units)\n",
    "\n",
    "    l_decoder_out = layers.DenseLayer(l_decoder4, num_units=input_dim,\n",
    "                                       nonlinearity=nonlinearities.sigmoid)\n",
    "\n",
    "    x_to_z = LightweightModel([l_in], [l_latent])\n",
    "    x_to_y = LightweightModel([l_in], [l_observed])\n",
    "    z_to_x = LightweightModel([l_observed, l_latent], [l_decoder_out])\n",
    "    model = Model()\n",
    "    model.x_to_z = x_to_z\n",
    "    model.x_to_y = x_to_y\n",
    "    model.z_to_x = z_to_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if model_type == \"convnet\":\n",
    "    Layers = dict()\n",
    "    deconv = True\n",
    "    ## CNN\n",
    "    latent_size = 10\n",
    "    nb_filters=16\n",
    "    size_filters=5\n",
    "    nb_hidden=1000\n",
    "    nb_filters_encoder = nb_filters\n",
    "    nb_filters_decoder = nb_filters\n",
    "    size_filters_encoder = size_filters\n",
    "    size_filters_decoder = size_filters\n",
    "\n",
    "    l_in = layers.InputLayer((None, w*h))\n",
    "\n",
    "\n",
    "    x_in_reshaped = layers.ReshapeLayer(l_in, ([0], 1, w, h))\n",
    "\n",
    "    # conv1\n",
    "    l_conv = layers.Conv2DLayer(\n",
    "        x_in_reshaped,\n",
    "        num_filters=nb_filters_encoder,\n",
    "        filter_size=(size_filters_encoder, size_filters_encoder),\n",
    "        nonlinearity=nonlinearities.rectify,\n",
    "        name=\"conv1\",\n",
    "    )\n",
    "       \n",
    "    l_hid = layers.DenseLayer(\n",
    "        l_conv,\n",
    "        num_units=nb_hidden,\n",
    "        nonlinearity=nonlinearities.rectify,\n",
    "        name=\"hid1\"\n",
    "    )\n",
    "    \n",
    "    l_hid = layers.DenseLayer(\n",
    "        l_hid,\n",
    "        num_units=nb_hidden,\n",
    "        nonlinearity=nonlinearities.rectify,\n",
    "        name=\"pre_code_layer\"\n",
    "    )\n",
    "    l_hid = layers.DenseLayer(\n",
    "        l_hid,\n",
    "        num_units=nb_hidden,\n",
    "        nonlinearity=nonlinearities.rectify,\n",
    "        name=\"pre_code_layer\"\n",
    "    )\n",
    "        \n",
    "\n",
    "    #code layer\n",
    "\n",
    "    l_observed = layers.DenseLayer(l_hid, \n",
    "                                   num_units=output_dim,\n",
    "                                   nonlinearity=T.nnet.softmax,\n",
    "                                   name=\"observed\")\n",
    "\n",
    "    l_latent = layers.DenseLayer(l_hid, \n",
    "                                 num_units=latent_size,\n",
    "                                 nonlinearity=None,\n",
    "                                 name=\"factors\") # linear\n",
    "\n",
    "    hid = layers.ConcatLayer([l_latent, l_observed], axis=1, name=\"concat_layer\")\n",
    "\n",
    "    l_hid = layers.DenseLayer(\n",
    "        hid,\n",
    "        num_units=nb_hidden,\n",
    "        nonlinearity=nonlinearities.rectify,\n",
    "        name=\"hid3\"\n",
    "    )\n",
    "    \n",
    "    l_hid = layers.DenseLayer(\n",
    "        l_hid,\n",
    "        num_units=nb_hidden,\n",
    "        nonlinearity=nonlinearities.rectify,\n",
    "        name=\"hid4\"\n",
    "    )\n",
    "    \n",
    "        \n",
    " \n",
    "    if deconv is True:\n",
    "\n",
    "        # unflatten layer\n",
    "        hid = layers.DenseLayer(l_hid,\n",
    "                                num_units=nb_filters_decoder * (w + 1*(size_filters_decoder - 1)) * \n",
    "                                (h + 1*(size_filters_decoder - 1)),\n",
    "                                nonlinearity=nonlinearities.linear,\n",
    "                                name=\"unflatten\")\n",
    "        hid = layers.ReshapeLayer(hid,\n",
    "                                  ([0], nb_filters_decoder, \n",
    "                                   (w + 1*(size_filters_decoder - 1)), \n",
    "                                   (h + 1*(size_filters_decoder - 1))))\n",
    "\n",
    "        l_unconv = Conv2DLayer(\n",
    "            hid,\n",
    "            num_filters=nb_filters,\n",
    "            filter_size=(size_filters_decoder, size_filters_decoder),\n",
    "            nonlinearity=nonlinearities.linear,            \n",
    "            name=\"unconv1\"\n",
    "        )\n",
    "        #l_unconv = Conv2DLayer(\n",
    "        #    l_unconv,\n",
    "        #    num_filters=nb_filters,\n",
    "        #    filter_size=(size_filters_decoder, size_filters_decoder),\n",
    "        #    nonlinearity=nonlinearities.linear,\n",
    "        #    pad=\"full\",\n",
    "        #    name=\"unconv2\"\n",
    "        #)\n",
    "\n",
    "        l_unconv_sum = SumLayer(l_unconv, axis=1, name=\"sum\")\n",
    "        l_unconv_sum = layers.ReshapeLayer(l_unconv_sum, ([0], 1, [1], [2]))\n",
    "        #l_decoder_out = Conv2DLayer(\n",
    "        #    l_unconv_sum,\n",
    "        #    num_filters=1,\n",
    "        #    filter_size=(3, 3),\n",
    "        #    nonlinearity=nonlinearities.linear,\n",
    "        #    pad=\"same\",\n",
    "        #    name=\"smooth\"\n",
    "        #)\n",
    "        l_decoder_out = layers.ReshapeLayer(l_unconv_sum, ([0], w*h))\n",
    "        l_decoder_out = layers.NonlinearityLayer(l_decoder_out, nonlinearities.sigmoid, name=\"reconstruction\")\n",
    "        \n",
    "\n",
    "    else:\n",
    "        l_decoder_out = layers.DenseLayer(l_hid, \n",
    "                                          num_units=w*h, \n",
    "                                          nonlinearity=nonlinearities.sigmoid, name=\"reconstruction\")\n",
    "\n",
    "        \n",
    "\n",
    "    x_to_z = LightweightModel([l_in], [l_latent])\n",
    "    x_to_y = LightweightModel([l_in], [l_observed])\n",
    "    z_to_x = LightweightModel([l_observed, l_latent], [l_decoder_out])\n",
    "    model = Model()\n",
    "    model.x_to_z = x_to_z\n",
    "    model.x_to_y = x_to_y\n",
    "    model.z_to_x = z_to_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagnekit.misc.draw_net import draw_to_file\n",
    "draw_to_file(get_all_layers(l_decoder_out), \"model.svg\")\n",
    "\n",
    "from IPython.display import SVG\n",
    "SVG(\"model.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers_by_name = OrderedDict()\n",
    "\n",
    "\n",
    "all_layers = (\n",
    "    layers.get_all_layers(model.x_to_z.output_layers[0]) +\n",
    "    layers.get_all_layers(model.x_to_y.output_layers[0]) +\n",
    "    layers.get_all_layers(model.z_to_x.output_layers[0])\n",
    ")\n",
    "\n",
    "for l in all_layers:\n",
    "    if l.name is not None:\n",
    "        layers_by_name[l.name] = l\n",
    "print(layers_by_name.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the capsule object\n",
    "\n",
    "The Capsule object combines all the components:\n",
    "\n",
    "    - The model\n",
    "    - The training algorithm\n",
    "    - The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the Capsule object, which combines the Model/Training procedure/Loss function\n",
    "\n",
    "input_variables = OrderedDict()\n",
    "input_variables[\"X\"] = dict(tensor_type=T.matrix)\n",
    "input_variables[\"y\"] = dict(tensor_type=T.matrix)\n",
    "    \n",
    "\n",
    "functions = dict(\n",
    "    encode=dict(\n",
    "        get_output=lambda model, X:model.x_to_z.get_output(X)[0],\n",
    "        params=[\"X\"]\n",
    "    ),\n",
    "    predict=dict(\n",
    "        get_output=lambda model, X:(model.x_to_y.get_output(X)[0]).argmax(axis=1),\n",
    "        params=[\"X\"]\n",
    "    ),\n",
    "    reconstruct=dict(\n",
    "        get_output=lambda model, X: layers.get_output(l_decoder_out, X),\n",
    "        params=[\"X\"]\n",
    "    ),\n",
    "    get_reconstruction_error=dict(\n",
    "        get_output=lambda model, X: ((X-layers.get_output(l_decoder_out, X))**2).sum(axis=1).mean(),\n",
    "        params=[\"X\"]\n",
    "    ),\n",
    "    get_cross_correlation=dict(\n",
    "        get_output=lambda model, X: cross_correlation(model.x_to_z.get_output(X)[0],\n",
    "                                                      model.x_to_y.get_output(X)[0]),\n",
    "        params=[\"X\"]\n",
    "    ),\n",
    "    predict_proba=dict(\n",
    "        get_output=lambda model, X: model.x_to_y.get_output(X)[0],\n",
    "        params=[\"X\"]\n",
    "    ),\n",
    "    \n",
    "    get_all_layers=dict(\n",
    "        get_output=lambda model, X: [layers.get_output(l, X) for l in layers_by_name.values()],\n",
    "        params=[\"X\"]\n",
    "    ),\n",
    "    \n",
    "    #get_all_grads=dict(\n",
    "    #    get_output=lambda model, X, y: [theano.grad(loss_function(model, dict(X=X, y=y)), \n",
    "    #                                                p) for p in model.get_all_params()],\n",
    "    #    params=[\"X\", \"y\"]\n",
    "    #)\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "batch_optimizer = MyBatchOptimizer(\n",
    "    verbose=1,\n",
    "    max_nb_epochs=100,\n",
    "    batch_size=100,\n",
    "    optimization_procedure=(updates.rmsprop, \n",
    "                            {\"learning_rate\": 0.0001})\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(model, tensors):\n",
    "    x_to_z, x_to_y, z_to_x = model.x_to_z, model.x_to_y, model.z_to_x\n",
    "    X_batch, y_batch = tensors[\"X\"], tensors[\"y\"]\n",
    "    \n",
    "    z, = x_to_z.get_output(X_batch)\n",
    "    \n",
    "    \n",
    "    pre_code = layers.get_output(layers_by_name[\"pre_code_layer\"], X_batch)\n",
    "    \n",
    "    y_hat, = x_to_y.get_output(X_batch)\n",
    "    X_hat, = z_to_x.get_output(y_hat, z)\n",
    "    \n",
    "    loss_rec = ((X_hat - X_batch) ** 2).sum(axis=1).mean()\n",
    "    loss_supervised = ((y_hat - y_batch)**2).sum(axis=1).mean()\n",
    "    \n",
    "\n",
    " \n",
    "    return  (loss_rec + \n",
    "             10*loss_supervised + \n",
    "             15 * cross_correlation(z, y_hat))\n",
    "    \n",
    "capsule = Capsule(\n",
    "    input_variables, \n",
    "    model,\n",
    "    loss_function,\n",
    "    functions=functions,\n",
    "    batch_optimizer=batch_optimizer\n",
    ")\n",
    "\n",
    "Z_batch = T.matrix(\"z_batch\")\n",
    "capsule.decode = theano.function([Z_batch, capsule.v_tensors[\"y\"]], \n",
    "                                  layers.get_output(l_decoder_out,{l_latent: Z_batch, \n",
    "                                                    l_observed: capsule.v_tensors[\"y\"]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "capsule._build_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "capsule.fit(X=X[train], y=y[train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagnekit.misc.plot_weights import grid_plot\n",
    "\n",
    "if model_type == \"convnet\":\n",
    "    layers_enc = get_all_layers(model.x_to_z.output_layers[0])\n",
    "    layers_dec = get_all_layers(model.z_to_x.output_layers[0])\n",
    "    for l in layers_enc[2], layers_dec[-4]:\n",
    "        plt.clf()\n",
    "        W = l.W.get_value()[:, 0]\n",
    "        grid_plot(W, imshow_options={\"cmap\": \"gray\"})\n",
    "        plt.show()\n",
    "elif model_type == \"fully_connected\":\n",
    "    layers_ = get_all_layers(l_decoder_out)\n",
    "    for W in (layers_[1].W.get_value().T, layers_[-1].W.get_value()):\n",
    "        plt.clf()\n",
    "        print(W.shape)\n",
    "        W = W.reshape((W.shape[0], w, h))\n",
    "        grid_plot(W, imshow_options={\"cmap\": \"gray\"}, nbrows=10, nbcols=10)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lasagnekit.easy import get_stat\n",
    "\n",
    "layer = 0\n",
    "train_stat = get_stat(\"rec_train\", capsule.batch_optimizer.stats)\n",
    "test_stat = get_stat(\"rec_valid\", capsule.batch_optimizer.stats)\n",
    "plt.yticks(np.arange(round(max(train_stat), 2), round(min(train_stat), 2), -1.5))\n",
    "plt.plot(train_stat, label=\"train\")\n",
    "plt.plot(test_stat, label=\"valid\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"reconstruction error\")\n",
    "plt.legend()\n",
    "plt.title(\"reconstruction\")\n",
    "plt.savefig(\"rec.png\")\n",
    "plt.show()\n",
    "\n",
    "epoch = get_stat(\"epoch\", capsule.batch_optimizer.stats)\n",
    "acc_train = get_stat(\"acc_train\", capsule.batch_optimizer.stats)\n",
    "acc_valid = get_stat(\"acc_valid\", capsule.batch_optimizer.stats)\n",
    "plt.title(\"accuracy\")\n",
    "plt.yticks(np.arange(round(min(acc_train), 2), round(max(acc_train), 2), 0.02))\n",
    "plt.plot(acc_train, label=\"train\")\n",
    "plt.plot(acc_valid, label=\"valid\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\"acc.png\")\n",
    "plt.show()\n",
    "\n",
    "epoch = get_stat(\"epoch\", capsule.batch_optimizer.stats)\n",
    "acc_train = get_stat(\"crosscor_train\", capsule.batch_optimizer.stats)\n",
    "acc_valid = get_stat(\"crosscor_valid\", capsule.batch_optimizer.stats)\n",
    "plt.title(\"crosscor\")\n",
    "plt.yticks(np.arange(round(max(acc_train), 3), round(min(acc_train), 3), -0.005))\n",
    "plt.plot(acc_train, label=\"train\")\n",
    "plt.plot(acc_valid, label=\"valid\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"crosscor\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\"crosscor.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.html.widgets import (interact, interactive, \n",
    "                                  IntSliderWidget, IntSlider, FloatSliderWidget,\n",
    "                                  ButtonWidget\n",
    "                                  )\n",
    "from IPython.display import display # Used to display widgets in the notebook\n",
    "\n",
    "from IPython.html.widgets import *\n",
    "from IPython.html import widgets\n",
    "\n",
    "\n",
    "use_examples = False # init/work with examples for dataset or not\n",
    "nb = 1000 # nb of examples to consider\n",
    "max_nb_sliders = 5\n",
    "T_ = test\n",
    "x = X[T_][0:nb]\n",
    "\n",
    "\n",
    "nb_outputs = y.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "z = capsule.encode(x)\n",
    "\n",
    "if z.shape[1] > max_nb_sliders:\n",
    "    params = np.random.choice(z.shape[1],\n",
    "                              size=10)\n",
    "else:\n",
    "    params = np.arange(z.shape[1])\n",
    "\n",
    "boundaries = OrderedDict()\n",
    "for p in (params):\n",
    "    boundaries[\"{0}\".format(p)] = FloatSliderWidget(min=z[:, p].min(),\n",
    "                                                    max=z[:, p].max(),\n",
    "                                                    step=0.01,\n",
    "                                                    value=0.)\n",
    "d = 0\n",
    "l = y[T_][d].argmax() \n",
    "\n",
    "def draw(**all_params):\n",
    "    if use_examples is True:\n",
    "        example = all_params[\"example\"]\n",
    "        del all_params[\"example\"]\n",
    "    label = all_params[\"label\"]\n",
    "    del all_params[\"label\"]\n",
    "    params = all_params\n",
    "    \n",
    "    if use_examples is True:\n",
    "        z = capsule.encode(x[example:example + 1])\n",
    "    else:\n",
    "        z = np.zeros((1, latent_size), dtype=\"float32\")\n",
    "        z[0, :] = 0\n",
    "    \n",
    "    y_ = np.zeros(nb_outputs, dtype='float32')\n",
    "    y_[label] = 1.\n",
    "    y_ = y_[np.newaxis, :]\n",
    "        \n",
    "    for k, v in params.items():\n",
    "        z[0][int(k)] = v\n",
    "    plt.imshow(capsule.decode(z, y_)[0].reshape((w, h)), cmap=\"gray\")\n",
    "    \n",
    "\n",
    "p = dict()\n",
    "p.update(boundaries)\n",
    "\n",
    "label_selector = IntSliderWidget(min=0,max=26,step=1,value=l)\n",
    "p[\"label\"] = label_selector\n",
    "\n",
    "if use_examples is True:\n",
    "    example_selector = IntSliderWidget(min=0,max=nb-1,step=1,value=d)\n",
    "    p[\"example\"] = example_selector\n",
    "\n",
    "i = interact(**p)\n",
    "\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    \n",
    "    example = example_selector.get_state()[\"value\"]\n",
    "    z = capsule.encode(x[example:example + 1])\n",
    "    for p in params:\n",
    "        w = boundaries[\"{0}\".format(p)]\n",
    "        state = w.get_state()\n",
    "        state[\"value\"] = z[0, int(p)]\n",
    "        w.set_state(state)\n",
    "        w.send_state(state)\n",
    "        \n",
    "    state = label_selector.get_state()\n",
    "    state[\"value\"] = y[example].argmax()\n",
    "    label_selector.set_state(state)\n",
    "    label_selector.send_state(state)\n",
    "    \n",
    "    \n",
    "draw_i = i(draw)\n",
    "\n",
    "if use_examples is True:\n",
    "    button = widgets.ButtonWidget(description=\"fit!\")\n",
    "    display(button)\n",
    "    button.on_click(on_button_clicked)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sharpness(images):\n",
    "    _, gy, gx = np.gradient(images)\n",
    "    return (np.sqrt(gx**2 + gy**2)).mean(axis=(1, 2)).mean()\n",
    "\n",
    "nb = 10000\n",
    "x_ = X[test][0:nb]\n",
    "x_hat_ = capsule.reconstruct(x_)\n",
    "\n",
    "x_ = x_.reshape((x_.shape[0], w, h))\n",
    "x_hat_ = x_hat_.reshape((x_hat_.shape[0], w, h))\n",
    "\n",
    "# lower = less sharp, more blurry\n",
    "print(\"original\", sharpness(x_))\n",
    "print(\"reconstruction\", sharpness(x_hat_))\n",
    "print(sharpness(x_hat_)/sharpness(x_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaion of reconstruction of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = 100\n",
    "x_ = X[test][0:nb]\n",
    "x_hat_ = capsule.reconstruct(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(nb):\n",
    "    plt.clf()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(x_[i].reshape((w, h)), cmap=\"gray\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(x_hat_[i].reshape((w, h)), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d histogram of hidden factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SOurce :http://oceanpython.org/2013/02/25/2d-histogram/\n",
    "x_ = X[0:20000]\n",
    "z = capsule.encode(x_)\n",
    "\n",
    "# Estimate the 2D histogram\n",
    "nbins = 200\n",
    "H, xedges, yedges = np.histogram2d(z[:, 0],z[:, 1],bins=nbins)\n",
    " \n",
    "# H needs to be rotated and flipped\n",
    "H = np.rot90(H)\n",
    "H = np.flipud(H)\n",
    " \n",
    "# Mask zeros\n",
    "Hmasked = np.ma.masked_where(H==0,H) # Mask pixels with a value of zero\n",
    " \n",
    "# Plot 2D histogram using pcolor\n",
    "fig2 = plt.figure()\n",
    "plt.pcolormesh(xedges,yedges,Hmasked, cmap=\"afmhot\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the gaussianity of the hidden factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosistest\n",
    "import numpy as np\n",
    "x_ = X[0:10000]\n",
    "z = capsule.encode(x_)\n",
    "\n",
    "_, pvalues = kurtosistest(z)\n",
    "print(pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance matrix of hidden factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_ = X[0:5000]\n",
    "z = capsule.encode(x_)\n",
    "print(z.mean(axis=0))\n",
    "plt.matshow(np.abs(np.corrcoef(z.T)))\n",
    "plt.colorbar()\n",
    "np.save(\"hidfact_corr{0}_{1}\".format(latent_size, model_type), np.corrcoef(z.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent order by variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_ = X[0:5000]\n",
    "z = capsule.encode(x_)\n",
    "\n",
    "C = np.cov(z.T)\n",
    "latent_order = np.argsort(np.diag(C))[::-1]\n",
    "print(latent_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of samples when varying hidden factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lasagnekit.misc.plot_weights import grid_plot\n",
    "nb = 10# nb of different hidden factors values to consider\n",
    "labels = np.arange(output_dim)# labels to consider (by default, all)\n",
    "labels = np.arange(10)\n",
    "std_units=4# nb of std units of values of latent dim around the mean to consider\n",
    "L = latent_size\n",
    "\n",
    "x_ = X[train][0:100]\n",
    "z_ = capsule.encode(x_)\n",
    "latent_std = np.std(z_, axis=0)\n",
    "latent_mean = np.mean(z_, axis=0)\n",
    "\n",
    "k = 1\n",
    "for latent_dim in latent_order:\n",
    "    print(\"hidden factor : {0}\".format(latent_dim))\n",
    "    ys = np.eye(output_dim)[labels].repeat(nb, axis=0)\n",
    "    seq = np.linspace(latent_mean[latent_dim] - latent_std[latent_dim]*std_units,\n",
    "                      latent_mean[latent_dim] + latent_std[latent_dim]*std_units,\n",
    "                      nb)\n",
    "    z = np.zeros((nb, L))\n",
    "    z[:, latent_dim] = seq\n",
    "    z = z.repeat(len(labels), axis=0)\n",
    "    z = z.reshape((nb, len(labels), L))\n",
    "    z = z.transpose((1, 0, 2))\n",
    "    z = z.reshape((nb*len(labels), L))\n",
    "    z = z.astype(np.float32)\n",
    "    ys = ys.astype(np.float32)\n",
    "\n",
    "    c = capsule.decode(z, ys)\n",
    "    c = c.reshape((c.shape[0], w, h))\n",
    "    plt.clf()\n",
    "    grid_plot(c, imshow_options=dict(cmap=\"gray\"), nbrows=len(labels), nbcols=nb)\n",
    "    plt.savefig(\"hidfactor{0}.png\".format(k))\n",
    "    plt.show()\n",
    "    k += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
